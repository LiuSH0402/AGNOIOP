{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# **Lazy Loading: Load data on demand**\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, root, label, transform=None, pre_transform=None):\n",
    "        super(GraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.file_paths = [os.path.join(root, f) for f in os.listdir(root)]  # Store file paths only to save memory\n",
    "        self.label = label\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" Load a single sample on demand to avoid loading all data at once \"\"\"\n",
    "        file_path = self.file_paths[idx]\n",
    "        data = torch.load(file_path, weights_only=False)  # Load only when needed\n",
    "        data.y = torch.tensor([self.label], dtype=torch.float32)\n",
    "        return data\n",
    "\n",
    "# **Data loading**\n",
    "def load_data(data_dir):\n",
    "    datasets = {}\n",
    "    for split in ['neg-test', 'neg-train', 'pos-test', 'pos-train']:\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        if os.path.exists(split_dir):\n",
    "            label = 0 if 'neg' in split else 1\n",
    "            datasets[split] = GraphDataset(root=split_dir, label=label)\n",
    "        else:\n",
    "            print(f\"Warning: {split_dir} does not exist.\")\n",
    "    return datasets\n",
    "\n",
    "# **Get k-fold data loaders**\n",
    "def get_kfold_data(data_dir, n_splits=5):\n",
    "    datasets = load_data(data_dir)\n",
    "\n",
    "    # Training dataset\n",
    "    train_dataset = datasets['neg-train'] + datasets['pos-train']\n",
    "    # Testing dataset\n",
    "    test_dataset = datasets['neg-test'] + datasets['pos-test']\n",
    "\n",
    "    labels = [data.y.item() for data in train_dataset]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Generator yields training and validation sets for each fold\n",
    "    for train_idx, val_idx in skf.split(range(len(train_dataset)), labels):\n",
    "        train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=2, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_subset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "        yield train_loader, val_loader\n",
    "\n",
    "    # Finally return the full test set\n",
    "    return test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, global_max_pool, GATConv\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "\n",
    "##############  GNN Layer ##############\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, num_hidden, dropout=0.2, num_heads=8):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.ModuleList([nn.LayerNorm(num_hidden) for _ in range(2)])  # LayerNorm\n",
    "        self.attention = TransformerConv(\n",
    "            in_channels=num_hidden,\n",
    "            out_channels=int(num_hidden / num_heads),  \n",
    "            heads=num_heads,  \n",
    "            dropout=dropout,\n",
    "            edge_dim=num_hidden,  \n",
    "            root_weight=False\n",
    "        )\n",
    "        self.PositionWiseFeedForward = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_hidden * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden * 4, num_hidden)\n",
    "        )\n",
    "        self.edge_update = EdgeMLP(num_hidden, dropout)\n",
    "        self.context = Context(num_hidden)\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E, batch_id):\n",
    "\n",
    "        dh = self.attention(h_V, edge_index, h_E)  \n",
    "        h_V = self.norm[0](h_V + self.dropout(dh))  \n",
    "        dh = self.PositionWiseFeedForward(h_V)  \n",
    "        h_V = self.norm[1](h_V + self.dropout(dh))  \n",
    "        h_E = self.edge_update(h_V, edge_index, h_E) \n",
    "        h_V = self.context(h_V, edge_index)  \n",
    "        return h_V, h_E\n",
    "\n",
    "\n",
    "class EdgeMLP(nn.Module):\n",
    "    def __init__(self, num_hidden, dropout=0.2):\n",
    "        super(EdgeMLP, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.BatchNorm1d(num_hidden)\n",
    "        self.W11 = nn.Linear(3 * num_hidden, num_hidden, bias=True)\n",
    "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
    "        self.act = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E):\n",
    "        src_idx = edge_index[0] \n",
    "        dst_idx = edge_index[1]  \n",
    "        h_EV = torch.cat([h_V[src_idx], h_E, h_V[dst_idx]], dim=-1)  \n",
    "        h_message = self.W12(self.act(self.W11(h_EV)))  \n",
    "        h_E = self.norm(h_E + self.dropout(h_message))  \n",
    "        return h_E\n",
    "\n",
    "\n",
    "##############  Context (with GAT)  ##############\n",
    "class Context(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super(Context, self).__init__()\n",
    "        self.gat_conv = GATConv(in_channels=num_hidden, out_channels=num_hidden, heads=1, dropout=0.2)\n",
    "\n",
    "    def forward(self, h_V, edge_index):\n",
    "        h_V_context = self.gat_conv(h_V, edge_index) \n",
    "   \n",
    "        return h_V_context\n",
    "\n",
    "\n",
    "##############  Graph Encoder ##############\n",
    "class Graph_encoder(nn.Module):\n",
    "    def __init__(self, node_in_dim, edge_in_dim, hidden_dim, num_layers=4, drop_rate=0.2):\n",
    "        super(Graph_encoder, self).__init__()\n",
    "   \n",
    "        self.node_embedding = nn.Linear(node_in_dim, hidden_dim, bias=True)  \n",
    "        self.edge_embedding = nn.Linear(edge_in_dim, hidden_dim, bias=True)  \n",
    "        self.edge_transform = nn.Linear(edge_in_dim, hidden_dim)  \n",
    "        self.norm_nodes = nn.BatchNorm1d(hidden_dim)\n",
    "        self.norm_edges = nn.BatchNorm1d(hidden_dim)\n",
    "        self.W_v = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_e = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            GNNLayer(num_hidden=hidden_dim, dropout=drop_rate, num_heads=8)\n",
    "            for _ in range(num_layers)\n",
    "        )\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E, batch_id):\n",
    "\n",
    "        h_V = self.W_v(self.norm_nodes(self.node_embedding(h_V)))  \n",
    "\n",
    "        h_E = self.edge_transform(h_E)  \n",
    "        h_E = self.W_e(self.norm_edges(h_E))  \n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h_V, h_E = layer(h_V, edge_index, h_E, batch_id) \n",
    "        return h_V, h_E\n",
    "\n",
    "\n",
    "class GPSol(nn.Module):\n",
    "    def __init__(self, protein_node_input_dim, edge_input_dim, protein_hidden_dim, num_layers, dropout, device):\n",
    "        super(GPSol, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "        self.Graph_encoder_protein = Graph_encoder(\n",
    "            node_in_dim=protein_node_input_dim,  \n",
    "            edge_in_dim=edge_input_dim,  \n",
    "            hidden_dim=protein_hidden_dim,  \n",
    "            num_layers=num_layers,\n",
    "            drop_rate=dropout\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=2 * (protein_hidden_dim),  \n",
    "            nhead=4,  \n",
    "            num_encoder_layers=2,  \n",
    "            num_decoder_layers=2,  \n",
    "            dim_feedforward=1028, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  \n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(2 * protein_hidden_dim, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  \n",
    "        self.norm = nn.LayerNorm(2 * protein_hidden_dim)\n",
    "\n",
    "    def forward(self, h_V_protein, edge_index_protein, h_E_protein, batch_id_protein):\n",
    "\n",
    "        \n",
    "\n",
    "        h_V_protein, _ = self.Graph_encoder_protein(h_V_protein, edge_index_protein, h_E_protein, batch_id_protein)\n",
    "\n",
    "\n",
    "        h_V_protein_pooled = global_mean_pool(h_V_protein, batch_id_protein)  \n",
    "        h_V_protein_maxpooled = global_max_pool(h_V_protein, batch_id_protein)  \n",
    "        h_V_combined = torch.cat([h_V_protein_pooled, h_V_protein_maxpooled], dim=1)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        h_V_combined = h_V_combined.unsqueeze(0) \n",
    "        h_V_combined = self.transformer(h_V_combined, h_V_combined)  \n",
    "        h_V_combined = h_V_combined.squeeze(0) \n",
    "        \n",
    "\n",
    "        emb = F.relu(self.fc1(h_V_combined))  \n",
    "        emb = self.dropout(emb)  \n",
    "        output = torch.sigmoid(self.fc3(emb))  \n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# **Training function**\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        labels = batch.y.float().to(device) \n",
    "\n",
    "        h_V_protein = batch.x.to(device)\n",
    "        edge_index_protein = batch.edge_index.to(device)\n",
    "        h_E_protein = batch.edge_attr.to(device) if batch.edge_attr is not None else torch.zeros(batch.edge_index.size(1), 450).to(device)\n",
    "\n",
    "        output = model(h_V_protein, edge_index_protein, h_E_protein, batch.batch)\n",
    "        output = output.view(-1)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += ((output >= 0.5).float() == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        y_true.extend(labels.cpu().numpy())  \n",
    "        y_pred.extend(output.detach().cpu().numpy())\n",
    "\n",
    "    # Convert y_true and y_pred to NumPy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate AUC and AUPR from raw prediction scores\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    aupr = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    # Convert predictions to binary class labels\n",
    "    y_pred_labels = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    f1 = f1_score(y_true, y_pred_labels)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_labels)\n",
    "    precision = precision_score(y_true, y_pred_labels, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_labels, zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, auc, aupr, f1, mcc, precision, recall\n",
    "\n",
    "\n",
    "# **Validation function**\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            labels = batch.y.float().to(device)\n",
    "\n",
    "            h_V_protein = batch.x.to(device)\n",
    "            edge_index_protein = batch.edge_index.to(device)\n",
    "            h_E_protein = batch.edge_attr.to(device) if batch.edge_attr is not None else torch.zeros(batch.edge_index.size(1), 450).to(device)\n",
    "\n",
    "            output = model(h_V_protein, edge_index_protein, h_E_protein, batch.batch)\n",
    "            output = output.view(-1)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(output.detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    y_pred_labels = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = (y_pred_labels == y_true).mean()\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    aupr = average_precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred_labels)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred_labels)\n",
    "    precision = precision_score(y_true, y_pred_labels, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_labels, zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, auc, aupr, f1, mcc, precision, recall\n",
    "\n",
    "\n",
    "# **Train the final model directly**\n",
    "num_epochs = 100\n",
    "data_dir = r'D:\\UESTC\\odorant\\graph'  # Change to your data directory\n",
    "\n",
    "# Load all data\n",
    "train_data = load_data(data_dir)['neg-train'] + load_data(data_dir)['pos-train']\n",
    "test_data = load_data(data_dir)['neg-test'] + load_data(data_dir)['pos-test']\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = GPSol(\n",
    "    protein_node_input_dim=1217, \n",
    "    edge_input_dim=450, \n",
    "    protein_hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.5, \n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n",
    "# Directory to save models\n",
    "model_save_dir = r'D:\\UESTC\\odorant\\model'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "# Train the model and save model for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss, accuracy, auc, aupr, f1, mcc, precision, recall = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, AUPR: {aupr:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    # Validate after each epoch\n",
    "    val_loss, val_accuracy, val_auc, val_aupr, val_f1, val_mcc, val_precision, val_recall = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, AUC: {val_auc:.4f}, AUPR: {val_aupr:.4f}, F1: {val_f1:.4f}, MCC: {val_mcc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
    "\n",
    "    # Save the model of current epoch\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_dir, f'model_epoch_{epoch+1}.pth'))\n",
    "    print(f\"Model saved for epoch {epoch+1}.\")\n",
    "\n",
    "# **Load and evaluate the final test model**\n",
    "final_model = GPSol(\n",
    "    protein_node_input_dim=1217, \n",
    "    edge_input_dim=450, \n",
    "    protein_hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.5, \n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Load the final epoch model (change if needed)\n",
    "final_model.load_state_dict(torch.load(os.path.join(model_save_dir, f'model_epoch_{num_epochs}.pth')))\n",
    "final_model.eval()\n",
    "\n",
    "# **Final test evaluation**\n",
    "final_loss, final_acc, final_auc, final_aupr, final_f1, final_mcc, final_precision, final_recall = validate(final_model, test_loader, criterion, device)\n",
    "print(f\"Final Test Results: Loss: {final_loss:.4f}, Accuracy: {final_acc:.4f}, AUC: {final_auc:.4f}, AUPR: {final_aupr:.4f}, F1: {final_f1:.4f}, MCC: {final_mcc:.4f}, Precision: {final_precision:.4f}, Recall: {final_recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
