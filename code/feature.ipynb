{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "import typing as T\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "from esm.data import read_fasta\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Logging setup\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%y/%m/%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Helper functions\n",
    "def enable_cpu_offloading(model):\n",
    "    from torch.distributed.fsdp import CPUOffload, FullyShardedDataParallel\n",
    "    from torch.distributed.fsdp.wrap import enable_wrap, wrap\n",
    "\n",
    "    wrapper_kwargs = dict(cpu_offload=CPUOffload(offload_params=True))\n",
    "\n",
    "    with enable_wrap(wrapper_cls=FullyShardedDataParallel, **wrapper_kwargs):\n",
    "        for layer_name, layer in model.layers.named_children():\n",
    "            wrapped_layer = wrap(layer)\n",
    "            setattr(model.layers, layer_name, wrapped_layer)\n",
    "        model = wrap(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_model_on_gpu_with_cpu_offloading(model):\n",
    "    model = model.eval()\n",
    "    model_esm = enable_cpu_offloading(model.esm)\n",
    "    del model.esm\n",
    "    model.cuda()\n",
    "    model.esm = model_esm\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_batched_sequence_datasest(\n",
    "    sequences: T.List[T.Tuple[str, str]], max_tokens_per_batch: int = 1024\n",
    ") -> T.Generator[T.Tuple[T.List[str], T.List[str]], None, None]:\n",
    "    batch_headers, batch_sequences, num_tokens = [], [], 0\n",
    "    for header, seq in sequences:\n",
    "        if (len(seq) + num_tokens > max_tokens_per_batch) and num_tokens > 0:\n",
    "            yield batch_headers, batch_sequences\n",
    "            batch_headers, batch_sequences, num_tokens = [], [], 0\n",
    "        batch_headers.append(header)\n",
    "        batch_sequences.append(seq)\n",
    "        num_tokens += len(seq)\n",
    "\n",
    "    yield batch_headers, batch_sequences\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    if not args.fasta.exists():\n",
    "        raise FileNotFoundError(args.fasta)\n",
    "\n",
    "    args.pdb.mkdir(exist_ok=True)\n",
    "\n",
    "    # Read fasta and sort sequences by length\n",
    "    print(f\"Reading sequences from {args.fasta}\")\n",
    "    all_sequences = sorted(read_fasta(args.fasta), key=lambda header_seq: len(header_seq[1]))\n",
    "    print(f\"Loaded {len(all_sequences)} sequences from {args.fasta}\")\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    # Use pre-downloaded ESM weights from model_pth.\n",
    "    if args.model_dir is not None:\n",
    "        torch.hub.set_dir(args.model_dir)\n",
    "\n",
    "    model = esm.pretrained.esmfold_v1()\n",
    "\n",
    "    model = model.eval()\n",
    "    model.set_chunk_size(args.chunk_size)\n",
    "\n",
    "    if args.cpu_only:\n",
    "        model.esm.float()  # convert to fp32 as ESM-2 in fp16 is not supported on CPU\n",
    "        model.cpu()\n",
    "    elif args.cpu_offload:\n",
    "        model = init_model_on_gpu_with_cpu_offloading(model)\n",
    "    else:\n",
    "        model.cuda()\n",
    "    print(\"Starting Predictions\")\n",
    "    batched_sequences = create_batched_sequence_datasest(all_sequences, args.max_tokens_per_batch)\n",
    "\n",
    "    num_completed = 0\n",
    "    num_sequences = len(all_sequences)\n",
    "    for headers, sequences in batched_sequences:\n",
    "        start = timer()\n",
    "        try:\n",
    "            output = model.infer(sequences, num_recycles=args.num_recycles)\n",
    "        except RuntimeError as e:\n",
    "            if e.args[0].startswith(\"CUDA out of memory\"):\n",
    "                if len(sequences) > 1:\n",
    "                    print(\n",
    "                        f\"Failed (CUDA out of memory) to predict batch of size {len(sequences)}. \"\n",
    "                        \"Try lowering `--max-tokens-per-batch`.\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Failed (CUDA out of memory) on sequence {headers[0]} of length {len(sequences[0])}.\"\n",
    "                    )\n",
    "\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "        output = {key: value.cpu() for key, value in output.items()}\n",
    "        pdbs = model.output_to_pdb(output)\n",
    "        tottime = timer() - start\n",
    "        time_string = f\"{tottime / len(headers):0.1f}s\"\n",
    "        if len(sequences) > 1:\n",
    "            time_string = time_string + f\" (amortized, batch size {len(sequences)})\"\n",
    "        for header, seq, pdb_string, mean_plddt, ptm in zip(\n",
    "            headers, sequences, pdbs, output[\"mean_plddt\"], output[\"ptm\"]\n",
    "        ):\n",
    "            output_file = args.pdb / f\"{header}.pdb\"\n",
    "            output_file.write_text(pdb_string)\n",
    "            num_completed += 1\n",
    "            print(\n",
    "                f\"Predicted structure for {header} with length {len(seq)}, pLDDT {mean_plddt:0.1f}, \"\n",
    "                f\"pTM {ptm:0.3f} in {time_string}. \"\n",
    "                f\"{num_completed} / {num_sequences} completed.\"\n",
    "            )\n",
    "\n",
    "\n",
    "class Args:\n",
    "    fasta = Path(\"\")\n",
    "    pdb = Path(\"\")\n",
    "    model_dir = Path(\"\")  #\n",
    "    num_recycles = 6\n",
    "    max_tokens_per_batch = 256\n",
    "    chunk_size = 128\n",
    "    cpu_only = False\n",
    "    cpu_offload = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "\n",
    "run(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Set directory paths (modify to your actual local paths)\n",
    "pt_dir = ''\n",
    "surface_dir = ''\n",
    "output_dir = ''\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Traverse all .pt files\n",
    "for file in os.listdir(pt_dir):\n",
    "    if file.endswith('.pt'):\n",
    "        base = file.replace('.pt', '')  # e.g., \"0_positive_training\"\n",
    "        pt_path = os.path.join(pt_dir, file)\n",
    "\n",
    "        # Find the corresponding surface CSV file\n",
    "        surface_csv_name = f\"{base} dataset_surface.csv\"\n",
    "        surface_path = os.path.join(surface_dir, surface_csv_name)\n",
    "        \n",
    "        # Skip if the corresponding surface file is not found\n",
    "        if not os.path.exists(surface_path):\n",
    "            print(f\"⚠️ Surface file not found: {surface_csv_name}\")\n",
    "            continue\n",
    "\n",
    "        # Load graph features\n",
    "        data = torch.load(pt_path)\n",
    "        features = data['x']  # [num_nodes, 1217]\n",
    "\n",
    "        # Load exposed residue indices\n",
    "        surface_df = pd.read_csv(surface_path)\n",
    "        indices = torch.tensor(surface_df['Residue_Number'].astype(int).tolist()) - 1\n",
    "        surface_features = features[indices]\n",
    "\n",
    "        # Create output DataFrame with Residue_Number\n",
    "        output_df = pd.DataFrame(surface_features.numpy())\n",
    "        output_df.insert(0, 'Residue_Number', surface_df['Residue_Number'].values)\n",
    "\n",
    "        # Save the extracted features\n",
    "        out_csv_path = os.path.join(output_dir, f\"{base}_surface_node_features.csv\")\n",
    "        output_df.to_csv(out_csv_path, index=False)\n",
    "        print(f\"✅ Saved: {out_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch_geometric.nn import radius_graph\n",
    "import os\n",
    "from Bio import pairwise2\n",
    "\n",
    "# Process PDB file\n",
    "def get_pdb_xyz(pdb_file):\n",
    "    current_pos = -1000\n",
    "    X = []\n",
    "    current_aa = {}  # N, CA, C, O, R\n",
    "    for line in pdb_file:\n",
    "        if (line[0:4].strip() == \"ATOM\" and int(line[22:26].strip()) != current_pos) or line[0:4].strip() == \"TER\":\n",
    "            if current_aa != {}:\n",
    "                R_group = [current_aa[atom] for atom in current_aa if atom not in [\"N\", \"CA\", \"C\", \"O\"]]\n",
    "                R_group = np.array(R_group).mean(0) if R_group else current_aa[\"CA\"]\n",
    "                X.append([current_aa[\"N\"], current_aa[\"CA\"], current_aa[\"C\"], current_aa[\"O\"], R_group])\n",
    "                current_aa = {}\n",
    "            if line[0:4].strip() != \"TER\":\n",
    "                current_pos = int(line[22:26].strip())\n",
    "\n",
    "        if line[0:4].strip() == \"ATOM\":\n",
    "            atom = line[13:16].strip()\n",
    "            if atom != \"H\":\n",
    "                xyz = np.array([line[30:38].strip(), line[38:46].strip(), line[46:54].strip()]).astype(np.float32)\n",
    "                current_aa[atom] = xyz\n",
    "    return np.array(X)\n",
    "\n",
    "# Process DSSP file\n",
    "def process_dssp(dssp_file):\n",
    "    aa_type = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    SS_type = \"HBEGITSC\"\n",
    "    rASA_std = [115, 135, 150, 190, 210, 75, 195, 175, 200, 170,\n",
    "                185, 160, 145, 180, 225, 115, 140, 155, 255, 230]\n",
    "\n",
    "    with open(dssp_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    seq = \"\"\n",
    "    dssp_feature = []\n",
    "    \n",
    "    # Skip header\n",
    "    if lines[0].strip() == \"No.\\tAA\\tSS\\tASA (Angstrom**2)\\tRSA (%)\":\n",
    "        lines = lines[1:]\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        aa = parts[1]  # amino acid\n",
    "        SS = parts[2] if parts[2] != \" \" else \"C\"  # process secondary structure\n",
    "\n",
    "        if aa not in aa_type:\n",
    "            continue  # skip unknown amino acids\n",
    "\n",
    "        seq += aa\n",
    "        SS_vec = np.zeros(len(SS_type))\n",
    "        SS_vec[SS_type.find(SS)] = 1\n",
    "        \n",
    "        ASA = float(parts[3])\n",
    "        RSA = min(1, ASA / rASA_std[aa_type.find(aa)])\n",
    "        \n",
    "        dssp_feature.append(np.concatenate((np.array([RSA]), SS_vec)))\n",
    "\n",
    "    return seq, dssp_feature\n",
    "\n",
    "# Match DSSP sequence to reference sequence\n",
    "def match_dssp(seq, dssp, ref_seq):\n",
    "    alignments = pairwise2.align.globalxx(ref_seq, seq)\n",
    "    ref_seq = alignments[0].seqA\n",
    "    seq = alignments[0].seqB\n",
    "\n",
    "    padded_item = np.zeros(9)  # ensure same dimension as DSSP features\n",
    "\n",
    "    new_dssp = []\n",
    "    dssp_idx = 0\n",
    "    for aa in seq:\n",
    "        if aa == \"-\":\n",
    "            new_dssp.append(padded_item)\n",
    "        else:\n",
    "            new_dssp.append(dssp[dssp_idx])\n",
    "            dssp_idx += 1\n",
    "\n",
    "    matched_dssp = []\n",
    "    for i in range(len(ref_seq)):\n",
    "        if ref_seq[i] == \"-\":\n",
    "            continue\n",
    "        matched_dssp.append(new_dssp[i])\n",
    "\n",
    "    return matched_dssp\n",
    "\n",
    "# Compute geometric features\n",
    "def get_geo_feat(X, edge_index):\n",
    "    pos_embeddings = _positional_embeddings(edge_index)\n",
    "    node_angles = _get_angle(X)\n",
    "    node_dist, edge_dist = _get_distance(X, edge_index)\n",
    "    node_direction, edge_direction, edge_orientation = _get_direction_orientation(X, edge_index)\n",
    "\n",
    "    geo_node_feat = torch.cat([node_angles, node_dist, node_direction], dim=-1)\n",
    "    geo_edge_feat = torch.cat([pos_embeddings, edge_orientation, edge_dist, edge_direction], dim=-1)\n",
    "\n",
    "    return geo_node_feat, geo_edge_feat\n",
    "\n",
    "# Internal: positional encoding\n",
    "def _positional_embeddings(edge_index, num_embeddings=16):\n",
    "    d = edge_index[0] - edge_index[1]\n",
    "\n",
    "    frequency = torch.exp(\n",
    "        torch.arange(0, num_embeddings, 2, dtype=torch.float32, device=edge_index.device)\n",
    "        * -(np.log(10000.0) / num_embeddings)\n",
    "    )\n",
    "    angles = d.unsqueeze(-1) * frequency\n",
    "    PE = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n",
    "    return PE\n",
    "\n",
    "# Internal: get dihedral and bond angles\n",
    "def _get_angle(X, eps=1e-7):\n",
    "    X = torch.reshape(X[:, :3], [3 * X.shape[0], 3])\n",
    "    dX = X[1:] - X[:-1]\n",
    "    U = F.normalize(dX, dim=-1)\n",
    "    u_2 = U[:-2]\n",
    "    u_1 = U[1:-1]\n",
    "    u_0 = U[2:]\n",
    "\n",
    "    n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "    n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "\n",
    "    cosD = torch.sum(n_2 * n_1, -1)\n",
    "    cosD = torch.clamp(cosD, -1 + eps, 1 - eps)\n",
    "    D = torch.sign(torch.sum(u_2 * n_1, -1)) * torch.acos(cosD)\n",
    "    D = F.pad(D, [1, 2])\n",
    "    D = torch.reshape(D, [-1, 3])\n",
    "    dihedral = torch.cat([torch.cos(D), torch.sin(D)], 1)\n",
    "\n",
    "    cosD = (u_2 * u_1).sum(-1)\n",
    "    cosD = torch.clamp(cosD, -1 + eps, 1 - eps)\n",
    "    D = torch.acos(cosD)\n",
    "    D = F.pad(D, [1, 2])\n",
    "    D = torch.reshape(D, [-1, 3])\n",
    "    bond_angles = torch.cat((torch.cos(D), torch.sin(D)), 1)\n",
    "\n",
    "    node_angles = torch.cat((dihedral, bond_angles), 1)\n",
    "    return node_angles\n",
    "\n",
    "# Internal: radial basis function for distances\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16):\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=D.device).view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n",
    "# Internal: compute distances\n",
    "def _get_distance(X, edge_index):\n",
    "    atom_N = X[:, 0]\n",
    "    atom_Ca = X[:, 1]\n",
    "    atom_C = X[:, 2]\n",
    "    atom_O = X[:, 3]\n",
    "    atom_R = X[:, 4]\n",
    "\n",
    "    node_list = ['Ca-N', 'Ca-C', 'Ca-O', 'N-C', 'N-O', 'O-C', 'R-N', 'R-Ca', \"R-C\", 'R-O']\n",
    "    node_dist = []\n",
    "    for pair in node_list:\n",
    "        atom1, atom2 = pair.split('-')\n",
    "        E_vectors = vars()['atom_' + atom1] - vars()['atom_' + atom2]\n",
    "        rbf = _rbf(E_vectors.norm(dim=-1))\n",
    "        node_dist.append(rbf)\n",
    "    node_dist = torch.cat(node_dist, dim=-1)\n",
    "\n",
    "    atom_list = [\"N\", \"Ca\", \"C\", \"O\", \"R\"]\n",
    "    edge_dist = []\n",
    "    for atom1 in atom_list:\n",
    "        for atom2 in atom_list:\n",
    "            E_vectors = vars()['atom_' + atom1][edge_index[0]] - vars()['atom_' + atom2][edge_index[1]]\n",
    "            rbf = _rbf(E_vectors.norm(dim=-1))\n",
    "            edge_dist.append(rbf)\n",
    "    edge_dist = torch.cat(edge_dist, dim=-1)\n",
    "\n",
    "    return node_dist, edge_dist\n",
    "\n",
    "# Internal: direction and orientation features\n",
    "def _get_direction_orientation(X, edge_index):\n",
    "    X_N = X[:, 0]\n",
    "    X_Ca = X[:, 1]\n",
    "    X_C = X[:, 2]\n",
    "    u = F.normalize(X_Ca - X_N, dim=-1)\n",
    "    v = F.normalize(X_C - X_Ca, dim=-1)\n",
    "    b = F.normalize(u - v, dim=-1)\n",
    "    n = F.normalize(torch.cross(u, v), dim=-1)\n",
    "    local_frame = torch.stack([b, n, torch.cross(b, n)], dim=-1)\n",
    "\n",
    "    node_j, node_i = edge_index\n",
    "\n",
    "    t = F.normalize(X[:, [0, 2, 3, 4]] - X_Ca.unsqueeze(1), dim=-1)\n",
    "    node_direction = torch.matmul(t, local_frame).reshape(t.shape[0], -1)\n",
    "\n",
    "    t = F.normalize(X[node_j] - X_Ca[node_i].unsqueeze(1), dim=-1)\n",
    "    edge_direction_ji = torch.matmul(t, local_frame[node_i]).reshape(t.shape[0], -1)\n",
    "    t = F.normalize(X[node_i] - X_Ca[node_j].unsqueeze(1), dim=-1)\n",
    "    edge_direction_ij = torch.matmul(t, local_frame[node_j]).reshape(t.shape[0], -1)\n",
    "    edge_direction = torch.cat([edge_direction_ji, edge_direction_ij], dim=-1)\n",
    "\n",
    "    r = torch.matmul(local_frame[node_i].transpose(-1, -2), local_frame[node_j])\n",
    "    edge_orientation = _quaternions(r)\n",
    "\n",
    "    return node_direction, edge_direction, edge_orientation\n",
    "\n",
    "# Internal: convert rotation matrix to quaternion\n",
    "def _quaternions(R):\n",
    "    diag = torch.diagonal(R, dim1=-2, dim2=-1)\n",
    "    Rxx, Ryy, Rzz = diag.unbind(-1)\n",
    "    magnitudes = 0.5 * torch.sqrt(torch.abs(1 + torch.stack([\n",
    "        Rxx - Ryy - Rzz,\n",
    "        -Rxx + Ryy - Rzz,\n",
    "        -Rxx - Ryy + Rzz\n",
    "    ], -1)))\n",
    "    _R = lambda i, j: R[:, i, j]\n",
    "    signs = torch.sign(torch.stack([\n",
    "        _R(2, 1) - _R(1, 2),\n",
    "        _R(0, 2) - _R(2, 0),\n",
    "        _R(1, 0) - _R(0, 1)\n",
    "    ], -1))\n",
    "    xyz = signs * magnitudes\n",
    "    w = torch.sqrt(F.relu(1 + diag.sum(-1, keepdim=True))) / 2.\n",
    "    Q = torch.cat((xyz, w), -1)\n",
    "    Q = F.normalize(Q, dim=-1)\n",
    "    return Q\n",
    "\n",
    "# Dataset class\n",
    "class GPSite_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pdb_path, dssp_path, transpro_path, radius=15):\n",
    "        super(GPSite_Dataset, self).__init__()\n",
    "        self.pdb_path = pdb_path\n",
    "        self.dssp_path = dssp_path\n",
    "        self.transpro_path = transpro_path\n",
    "        self.radius = radius\n",
    "        self.IDs = self._get_ids()\n",
    "        self.failed_samples = []  # store failed samples and reasons\n",
    "\n",
    "    def _get_ids(self):\n",
    "        return [f.split('.')[0] for f in os.listdir(self.pdb_path) if f.endswith('.pdb')]\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.IDs)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        try:\n",
    "            return self._featurize_graph(idx)\n",
    "        except Exception as e:\n",
    "            sample_id = self.IDs[idx]\n",
    "            self.failed_samples.append((sample_id, str(e)))\n",
    "            print(f\"⚠️ Skipped sample {sample_id}, reason: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _featurize_graph(self, idx):\n",
    "        name = self.IDs[idx]\n",
    "        with torch.no_grad():\n",
    "            # Load PDB file\n",
    "            with open(os.path.join(self.pdb_path, f\"{name}.pdb\"), 'r') as pdb_file:\n",
    "                X = torch.tensor(get_pdb_xyz(pdb_file.readlines()), dtype=torch.float32)\n",
    "\n",
    "            # Load DSSP file\n",
    "            dssp_seq, dssp_feat = process_dssp(os.path.join(self.dssp_path, f\"{name}.dssp\"))\n",
    "            dssp_feat = torch.tensor(dssp_feat, dtype=torch.float32)\n",
    "\n",
    "            # Load TransPro file\n",
    "            transpro_feat = torch.load(os.path.join(self.transpro_path, f\"{name}.tensor\")).clone().detach()\n",
    "            if transpro_feat.shape[1] != 1024:\n",
    "                raise ValueError(f\"Expected TransPro feature dimension to be 1024, but got {transpro_feat.shape[1]}.\")\n",
    "\n",
    "            # Compute geometry features\n",
    "            edge_index = radius_graph(X[:, 1], r=self.radius)\n",
    "            geo_node_feat, geo_edge_feat = get_geo_feat(X, edge_index)\n",
    "\n",
    "            # Truncate all node features to the shortest length\n",
    "            min_nodes = min(dssp_feat.shape[0], geo_node_feat.shape[0], transpro_feat.shape[0])\n",
    "            dssp_feat = dssp_feat[:min_nodes]\n",
    "            geo_node_feat = geo_node_feat[:min_nodes]\n",
    "            transpro_feat = transpro_feat[:min_nodes]\n",
    "\n",
    "            pre_computed_node_feat = torch.cat([dssp_feat, geo_node_feat, transpro_feat], dim=-1)\n",
    "\n",
    "            # Normalize node features\n",
    "            node_mean = pre_computed_node_feat.mean(dim=0, keepdim=True)\n",
    "            node_std = pre_computed_node_feat.std(dim=0, keepdim=True)\n",
    "            pre_computed_node_feat = (pre_computed_node_feat - node_mean) / (node_std + 1e-6)\n",
    "\n",
    "            # Normalize edge features\n",
    "            edge_features = geo_edge_feat\n",
    "            edge_mean = edge_features.mean(dim=0, keepdim=True)\n",
    "            edge_std = edge_features.std(dim=0, keepdim=True)\n",
    "            edge_features = (edge_features - edge_mean) / (edge_std + 1e-6)\n",
    "\n",
    "        graph_data = Data(name=name, x=pre_computed_node_feat, edge_index=edge_index, edge_attr=edge_features)\n",
    "        return graph_data\n",
    "\n",
    "    def save_graphs(self, output_dir):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        for idx in range(len(self)):\n",
    "            graph_data = self[idx]\n",
    "            if graph_data is None:\n",
    "                continue  # Skip failed samples\n",
    "            \n",
    "            file_path = os.path.join(output_dir, f\"{graph_data.name}.pt\")\n",
    "            torch.save(graph_data, file_path)\n",
    "            print(f\"✅ Saved graph: {file_path}\")\n",
    "\n",
    "        # Print failed samples\n",
    "        if self.failed_samples:\n",
    "            print(\"\\n❌ The following samples failed:\")\n",
    "            for sample_id, error in self.failed_samples:\n",
    "                print(f\"- {sample_id}: {error}\")\n",
    "\n",
    "# Example usage\n",
    "pdb_folder = '/home/shliu/odorant/yanzhen/PDB/'\n",
    "dssp_folder = '/home/shliu/odorant/yanzhen/dssp/'\n",
    "transpro_folder = '/home/shliu/odorant/yanzhen/prot/ProtTrans/'\n",
    "output_folder = '/home/shliu/odorant/yanzhen/graph/'\n",
    "\n",
    "dataset = GPSite_Dataset(pdb_folder, dssp_folder, transpro_folder)\n",
    "dataset.save_graphs(output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
